# 2025年10月6日
现有框架实现了CartPole-v1环境下的DQN，REINFORCE算法（及其基线版本），但是仍然存在部分问题，现做如下说明
## 阶段问题描述
1. 没有办法直接拓展到其他游戏环境.根本原因是因为不同游戏环境的state的维度等不一定一致，虽然动态获取了state的维度，但是如果state是高维数组，仍然不
便于处理，例如大量运用在各个神经网络中的线性层nn.Linear，由于其默认只处理最后一个维度，所以在处理高维数组时一定存在逻辑问题，且高维数组往往需要进行预
处理之后才便于传入神经网络（无论神经网络是价值函数还是策略函数）
2. 没有较好地提供时序相关的处理能力，在Atari游戏中，有时会遇到一些需要处理时序数据的环境，例如Pong-v5，在处理时序相关的观察时，有两种做法，一种是堆
叠帧，并将堆叠多个时间刻的帧作为一整个state输入；另外一种是在对应的神经网络中使用RNN，GRU或者LSTM等时序计算单元.使用帧堆叠的好处是简单，但是帧堆叠
只能处理有限的帧的时序依赖问题，不能解决需要长期记忆的问题；使用RNN、GRU或者LSTM会极大增加计算量.  
然而在目前的框架中，并没有为帧堆叠和时序依赖模型的引入留足接口，这是一大隐患.
3. 打印的信息和时机不够完善，只打印了episode的return，没有记录其他数据，例如loss，梯度模，选择动作的Q值。
4. 没有进行测试，没有加载和保存模型的逻辑
## 阶段解决思路
1. 引入一个新的模块，对于任何一个环境，该模块下存在一个单元（配置一个默认单元，用于处理没有在该模块下单独配置一个单元的环境），该单元用于对其对应的环境
返回的state的进行预处理
2. 引入新的参数，如帧堆叠的数量，并且修改部分接口的标准，例如，controller的